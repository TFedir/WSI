{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 4 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from statistics import mode\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "def entropy_func(class_count, num_samples):\n",
        "    p = class_count / num_samples\n",
        "    return -1 * (p * np.log(p))\n",
        "\n",
        "\n",
        "class Group:\n",
        "    def __init__(self, group_classes):\n",
        "        self.group_classes = group_classes\n",
        "        self.entropy = self.group_entropy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.group_classes.size\n",
        "\n",
        "    def group_entropy(self):\n",
        "        entropy = 0\n",
        "        for group in set(self.group_classes):\n",
        "            entropy += entropy_func(dict(zip(*np.unique(self.group_classes, return_counts=True))).get(group), len(self.group_classes))\n",
        "        return entropy\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, split_feature, split_val, depth=None, child_node_a=None, child_node_b=None, val=None):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_val = split_val\n",
        "        self.depth = depth\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val\n",
        "\n",
        "    def predict(self, data):\n",
        "        if self.val is not None:\n",
        "            return self.val\n",
        "        return self.child_node_a.predict(data) if data[self.split_feature] < self.split_val else self.child_node_b.predict(data)\n",
        "\n",
        "\n",
        "class DecisionTreeClassifier(object):\n",
        "    def __init__(self, max_depth):\n",
        "        self.depth = 0\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_split_entropy(group_a:Group, group_b:Group):\n",
        "        length = len(group_a.group_classes) + len(group_b.group_classes)\n",
        "        a_ratio = len(group_a.group_classes) / length\n",
        "        b_ratio = len(group_b.group_classes) / length\n",
        "        return a_ratio * group_a.entropy + b_ratio * group_b.entropy\n",
        "\n",
        "    def get_information_gain(self, parent_group:Group, child_group_a:Group, child_group_b:Group):\n",
        "        return parent_group.entropy - self.get_split_entropy(child_group_a, child_group_b)\n",
        "\n",
        "    def get_best_feature_split(self, feature_values, classes):\n",
        "        unique_values = set(feature_values)\n",
        "        parent_group = Group(classes)\n",
        "        best_split, best_gain = None, 0\n",
        "        for val in unique_values:\n",
        "            # spliting the parent group into 2 groups, dividing by val\n",
        "            smaller_values = [feature_values[i] for i in range(len(feature_values)) if feature_values[i] < val]\n",
        "            smaller_classes = [classes[i] for i in range(len(feature_values)) if feature_values[i] < val]\n",
        "            bigger_values = [feature_values[i] for i in range(len(feature_values)) if feature_values[i] >= val]\n",
        "            bigger_classes = [classes[i] for i in range(len(feature_values)) if feature_values[i] >= val]\n",
        "            child_group_a, child_group_b = Group(smaller_classes), Group(bigger_classes)\n",
        "            gain = self.get_information_gain(parent_group, child_group_a, child_group_b)\n",
        "            if gain > best_gain:\n",
        "                best_split, best_gain = val, gain\n",
        "        return (best_gain, best_split)\n",
        "\n",
        "    def get_best_split(self, data, classes):\n",
        "        best_split, split_value, best_gain = None, None, 0\n",
        "        for feature in range(len(data[0])):\n",
        "            feature_values = [row[feature] for row in data]\n",
        "            gain, split = self.get_best_feature_split(feature_values, classes)\n",
        "            if gain > best_gain:\n",
        "                best_split, split_value, best_gain = feature, split, gain\n",
        "        return (best_split, split_value)\n",
        "\n",
        "    def build_tree(self, data, classes, depth=0):\n",
        "        self.tree = self._build_tree_recursively(data, classes, depth)\n",
        "        return self.tree\n",
        "\n",
        "    def _build_tree_recursively(self, data, classes, depth):\n",
        "        if len(set(classes)) == 1:\n",
        "            return Node(None, None, 0, None, None, classes[0])\n",
        "        if depth == 0:\n",
        "            # return Node containing most frequent value in classes\n",
        "            return Node(None, None, 0, None, None, mode(classes))\n",
        "        best_split, split_value = self.get_best_split(data, classes)\n",
        "        child_a_data = [data[i] for i in range(len(data)) if data[i][best_split] < split_value]\n",
        "        child_a_classes = [classes[i] for i in range(len(data)) if data[i][best_split] < split_value]\n",
        "        child_b_data = [data[i] for i in range(len(data)) if data[i][best_split] >= split_value]\n",
        "        child_b_classes = [classes[i] for i in range(len(data)) if data[i][best_split] >= split_value]        \n",
        "        return Node(best_split, split_value, depth, self._build_tree_recursively(child_a_data, child_a_classes, depth-1), self._build_tree_recursively(child_b_data, child_b_classes, depth-1))\n",
        "\n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "id": "U033RY1_YS8x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc = 0.9777777777777777\n"
          ]
        }
      ],
      "source": [
        "dc = DecisionTreeClassifier(3)\n",
        "dc.build_tree(x_train, y_train, 3)\n",
        "correct = 0\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    #print(f\"Prediction = {prediction} Actual = {gt}\")\n",
        "    correct += 1 if prediction == gt else 0\n",
        "acc = correct / y_test.size\n",
        "print(f\"Acc = {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nTest\\t\\n1\\t0.6\\t\\t\\t\\t\\n2\\t0.8667\\t\\t\\t\\t\\n3\\t0.9333\\n\\nTest size 0.15\\t\\t\\t\\t\\t  |  avg    \\n1\\t0,652\\t0,608\\t0,695\\t0,521 | 0,619\\n2\\t0,913\\t0,869\\t1\\t    0,913 |\\t0,923\\n3\\t0,913\\t0,956\\t0,826\\t1\\t  | 0,923\\n\\nTest size 0.2\\t\\t\\t\\t\\t\\n1\\t0,43\\t0,633\\t0,7\\t    0,6\\t  | 0,590\\n2\\t0,8\\t    1\\t    0,966\\t0,9\\t  | 0,916\\n3\\t0,866\\t0,9\\t    0,93\\t0,866 |\\t0,883\\n\\nTest size 0.3\\t\\t\\t\\t\\t\\n1\\t0,644\\t0,6\\t    0,577\\t0,711 | 0,633\\n2\\t0,955\\t0,977\\t0,866\\t0,933 |\\t0,932\\n3\\t0,9777\\t0,955\\t0,911\\t1\\t  | 0,961\\n'"
            ]
          },
          "execution_count": 363,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Test\t\n",
        "1\t0.6\t\t\t\t\n",
        "2\t0.8667\t\t\t\t\n",
        "3\t0.9333\n",
        "\n",
        "Test size 0.15\t\t\t\t\t  |  avg    \n",
        "1\t0,652\t0,608\t0,695\t0,521 | 0,619\n",
        "2\t0,913\t0,869\t1\t    0,913 |\t0,923\n",
        "3\t0,913\t0,956\t0,826\t1\t  | 0,923\n",
        "\n",
        "Test size 0.2\t\t\t\t\t\n",
        "1\t0,43\t0,633\t0,7\t    0,6\t  | 0,590\n",
        "2\t0,8\t    1\t    0,966\t0,9\t  | 0,916\n",
        "3\t0,866\t0,9\t    0,93\t0,866 |\t0,883\n",
        "\n",
        "Test size 0.3\t\t\t\t\t\n",
        "1\t0,644\t0,6\t    0,577\t0,711 | 0,633\n",
        "2\t0,955\t0,977\t0,866\t0,933 |\t0,932\n",
        "3\t0,9777\t0,955\t0,911\t1\t  | 0,961\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "822993cc2de8065dd07929654093e8c6598914b15835bcfeccbcac84530d789c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
