{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Zadanie 5\n",
    "\n",
    "\n",
    "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
    "\n",
    "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
    "\n",
    "Punktacja:\n",
    "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
    "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
    "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
    "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
    "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
    "    3. Wnioski 1.[5 pkt]\n",
    "4. Jakość kodu 0.[5 pkt]\n",
    "\n",
    "Polecane źródła - teoria + intuicja:\n",
    "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural layersworks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
    "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
    "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from sklearn import datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \"\"\"Basic building block of the Neural layerswork\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._learning_rate = 0.01\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x:np.ndarray)->np.ndarray:\n",
    "        \"\"\"Forward propagation of x through layer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error_derivative, learning_rate) ->np.ndarray:\n",
    "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "\n",
    "    @learning_rate.setter\n",
    "    def learning_rate(self, learning_rate):\n",
    "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
    "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    def __init__(self, input_size:int, output_size:int) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.input = None\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    def forward(self, x:np.ndarray)->np.ndarray:\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, output_error_derivative, learning_rate)->np.ndarray:\n",
    "        # dE/dx\n",
    "        error_of_input = np.dot(output_error_derivative, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error_derivative)\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error_derivative\n",
    "        # will be used as dE/dy by previous layer\n",
    "        return error_of_input\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x:np.ndarray)->np.ndarray:\n",
    "        self.input = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, output_error_derivative, learning_rate=None)->np.ndarray:\n",
    "        return  (1-np.tanh(self.input)**2) * output_error_derivative\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, loss_function:callable, loss_function_derivative:callable)->None:\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "\n",
    "    def loss(self, result_y, actual_y):\n",
    "        \"\"\"Loss function for a particular result_y and actual_y\"\"\"\n",
    "        return self.loss_function(result_y, actual_y)\n",
    "\n",
    "    def loss_derivative(self, result_y:np.ndarray, actual_y:np.ndarray)->np.ndarray:\n",
    "        \"\"\"Loss function derivative for a particular result_y and actual_y\"\"\"\n",
    "        derivative = self.loss_function_derivative(result_y, actual_y)\n",
    "        return derivative\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers:List[Layer], learning_rate:float)->None:\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = None\n",
    "\n",
    "    def compile(self, loss:Loss)->None:\n",
    "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
    "        self.loss = loss\n",
    "\n",
    "    def __call__(self, x:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
    "        input_for_next_layer = x\n",
    "        for layer in self.layers:\n",
    "            input_for_next_layer = layer.forward(input_for_next_layer)\n",
    "        return input_for_next_layer\n",
    "\n",
    "    def fit(self,\n",
    "            x_train:np.ndarray,\n",
    "            y_train:np.ndarray,\n",
    "            epochs:int,\n",
    "            verbose:int=0)->None:\n",
    "        \"\"\"Fit the layerswork to the training data\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            error = 0\n",
    "            for i in range(len(x_train)):\n",
    "                x = x_train[i]\n",
    "                #forward prop\n",
    "                result = self.__call__(x)\n",
    "                #for stats output only\n",
    "                for result_y, actual_y in zip(result, y_train[i]):\n",
    "                    error += self.loss.loss(result_y, actual_y)\n",
    "                output_error = self.loss.loss_derivative(result, y_train[i])\n",
    "                #backward prop\n",
    "                for layer in reversed(self.layers):\n",
    "                    output_error = layer.backward(output_error, self.learning_rate)\n",
    "\n",
    "            error /= len(x_train)\n",
    "            if verbose: print('epoch %d/%d   error=%f' % (epoch+1, epochs, error))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(result_y, actual_y):\n",
    "    # calculate mean squared error\n",
    "    return np.mean(np.power(actual_y - result_y, 2))\n",
    "\n",
    "def mse_derivative(result_y, actual_y):\n",
    "    derivative = 2*(result_y-actual_y)/len(actual_y)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FullyConnected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14908\\1248409444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mFullyConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFullyConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mxor_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxor_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse_derivative\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FullyConnected' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[[0,0]], [[0,1]], [[1,1]], [[1,0]]])\n",
    "y_train = np.array([[[0]], [[1]], [[0]], [[1]]])\n",
    "layers = [FullyConnected(2, 3), Tanh(), FullyConnected(3, 1), Tanh()]\n",
    "xor_net = Network(layers, 0.1)\n",
    "xor_net.compile(Loss(mse, mse_derivative))\n",
    "xor_net.fit(x_train, y_train, epochs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Eksperymenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = fetch_openml('mnist_784', version=1, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_as_vector(n):\n",
    "    return [1 if x==n else 0 for x in range(10)]\n",
    "\n",
    "def change_numbers_to_vectors(numbers):\n",
    "    new_numbers = []\n",
    "    for i in range(len(numbers)):\n",
    "        new_numbers.append(number_as_vector(numbers[i]))\n",
    "    return new_numbers\n",
    "\n",
    "def calculate_accuracy(net: Network, x_test, y_test):\n",
    "    samples = len(y_test)\n",
    "    correct = 0\n",
    "    for i in range(samples):\n",
    "        result = net.__call__(x_test[i])\n",
    "        max_probability = np.amax(result)\n",
    "        predicted_digit = (result.reshape((10))).tolist().index(max_probability)\n",
    "        correct += 1 if predicted_digit == y_test[i].reshape((10)).tolist().index(1) else 0\n",
    "    acc = correct/samples\n",
    "    return acc\n",
    "\n",
    "def prepare_data(x:np.array, y:np.array):\n",
    "    x = x.reshape(x.shape[0], 1, 28*28)\n",
    "    y = y.astype(np.int16)\n",
    "    vector_y = change_numbers_to_vectors(y)\n",
    "    y = np.reshape(vector_y, (y.shape[0], 1, 10))\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenowanie + testowanie modelu 1. Architektura: (784, 50) -> (50, 50) -> (50,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.7899642857142857\n",
      "Acc: 0.38648214285714283\n",
      "Acc: 0.34014285714285714\n"
     ]
    }
   ],
   "source": [
    "lr_rates = [0.001, 0.005, 0.01]\n",
    "for i in range(len(lr_rates)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        digits.data, digits.target, test_size=0.8\n",
    "    )\n",
    "    x_train, y_train = prepare_data(x=x_train, y=y_train)\n",
    "    layers = [FullyConnected(28*28, 50), Tanh(), FullyConnected(50, 50), Tanh(), FullyConnected(50, 10), Tanh()]\n",
    "    first_net = Network(layers, lr_rates[i])\n",
    "    first_net.compile(Loss(mse, mse_derivative))\n",
    "    first_net.fit(x_train, y_train, epochs=80, verbose=0)\n",
    "    # test\n",
    "    x_test, y_test = prepare_data(x_test, y_test)\n",
    "    acc = calculate_accuracy(first_net, x_test, y_test)\n",
    "    print(f\"Acc: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8385714285714285\n",
      "Acc: 0.8118392857142858\n",
      "Acc: 0.6892678571428571\n"
     ]
    }
   ],
   "source": [
    "lr_rates = [0.0005, 0.001, 0.002]\n",
    "for i in range(len(lr_rates)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        digits.data, digits.target, test_size=0.8\n",
    "    )\n",
    "    x_train, y_train = prepare_data(x=x_train, y=y_train)\n",
    "    layers = [FullyConnected(28*28, 50), Tanh(), FullyConnected(50, 50), Tanh(), FullyConnected(50, 10), Tanh()]\n",
    "    first_net = Network(layers, lr_rates[i])\n",
    "    first_net.compile(Loss(mse, mse_derivative))\n",
    "    first_net.fit(x_train, y_train, epochs=100, verbose=0)\n",
    "    # test\n",
    "    x_test, y_test = prepare_data(x_test, y_test)\n",
    "    acc = calculate_accuracy(first_net, x_test, y_test)\n",
    "    print(f\"Acc: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenowanie + testowanie modelu 2. Architektura: (784, 100) -> (100, 100) -> (100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc=0.7789285714285714\n",
      "Acc=0.715\n",
      "Acc=0.7285178571428571\n"
     ]
    }
   ],
   "source": [
    "lr_rates_for_wide = [0.0005, 0.001, 0.002]\n",
    "for learning_rate in lr_rates_for_wide:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        digits.data, digits.target, test_size=0.8\n",
    "    )\n",
    "    x_train, y_train = prepare_data(x=x_train, y=y_train)\n",
    "    layers = [FullyConnected(28*28, 100), Tanh(), FullyConnected(100, 100), Tanh(), FullyConnected(100, 10), Tanh()]\n",
    "    second_net = Network(layers, learning_rate)\n",
    "    second_net.compile(Loss(mse, mse_derivative))\n",
    "    second_net.fit(x_train, y_train, epochs=50)\n",
    "    # test\n",
    "    x_test, y_test = prepare_data(x_test, y_test)\n",
    "    acc = calculate_accuracy(second_net, x_test, y_test)\n",
    "    print(f\"Acc={acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenowanie + testowanie modelu 3. Architektura: (784, 50) -> (50, 50) -> (50, 50) -> (50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc=0.8368214285714286\n",
      "Acc=0.7799821428571428\n",
      "Acc=0.5724464285714286\n"
     ]
    }
   ],
   "source": [
    "lr_rates_for_long = [0.0005, 0.001, 0.002]\n",
    "for learning_rate in lr_rates_for_long:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        digits.data, digits.target, test_size=0.8\n",
    "    )\n",
    "    x_train, y_train = prepare_data(x=x_train, y=y_train)\n",
    "    layers = [FullyConnected(28*28, 50), Tanh(), FullyConnected(50, 50), Tanh(), FullyConnected(50, 50), Tanh(), FullyConnected(50, 10), Tanh()]\n",
    "    third_net = Network(layers, learning_rate)\n",
    "    third_net.compile(Loss(mse, mse_derivative))\n",
    "    third_net.fit(x_train, y_train, epochs=70)\n",
    "    # test\n",
    "    x_test, y_test = prepare_data(x_test, y_test)\n",
    "    acc = calculate_accuracy(third_net, x_test, y_test)\n",
    "    print(f\"Acc={acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wnioski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ilość epok pozytywnie wpływa na jakość sieci ale przy małych learning_rate, przy większych learning_rate będzie oscylować wokół jakiejś wartości.\n",
    "- Więcej neuronów nie znaczy lepsze wyniki\n",
    "- Więcej warstw zawsze pozytywnie wpływa na jakość sieci, ale potrzebują więcej epok dla wytrenowania. Przez to mogą dawać gorsze wyniki niż mniejsze sieci przy taką samej ilości epok\n",
    "- Dla sieci neuronowych hiperparametrami są ilość epok, learning_rate, glębokość i szerokość sieci."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "822993cc2de8065dd07929654093e8c6598914b15835bcfeccbcac84530d789c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
